{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement the Tree Algorithm with Credit Default Data \n",
    "\n",
    "# scikit learn package provide us the tree method for Categorical and Regression Tree\n",
    "from sklearn import tree\n",
    "X = [[0, 0], [1, 1]]\n",
    "Y = [0, 1]\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "clf.predict_proba([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python EDA](http://pythonplot.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"868pt\" height=\"642pt\"\n",
       " viewBox=\"0.00 0.00 867.83 642.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 638)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-638 863.8306,-638 863.8306,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"transparent\" stroke=\"#000000\" d=\"M523.3569,-634C523.3569,-634 384.3979,-634 384.3979,-634 378.3979,-634 372.3979,-628 372.3979,-622 372.3979,-622 372.3979,-568 372.3979,-568 372.3979,-562 378.3979,-556 384.3979,-556 384.3979,-556 523.3569,-556 523.3569,-556 529.3569,-556 535.3569,-562 535.3569,-568 535.3569,-568 535.3569,-622 535.3569,-622 535.3569,-628 529.3569,-634 523.3569,-634\"/>\n",
       "<text text-anchor=\"start\" x=\"380.3877\" y=\"-618.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) ≤ 2.45</text>\n",
       "<text text-anchor=\"start\" x=\"413.5933\" y=\"-604.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.6667</text>\n",
       "<text text-anchor=\"start\" x=\"408.1553\" y=\"-590.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 150</text>\n",
       "<text text-anchor=\"start\" x=\"394.1382\" y=\"-576.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [50, 50, 50]</text>\n",
       "<text text-anchor=\"start\" x=\"409.3276\" y=\"-562.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"#000000\" d=\"M423.7837,-513C423.7837,-513 327.9712,-513 327.9712,-513 321.9712,-513 315.9712,-507 315.9712,-501 315.9712,-501 315.9712,-461 315.9712,-461 315.9712,-455 321.9712,-449 327.9712,-449 327.9712,-449 423.7837,-449 423.7837,-449 429.7837,-449 435.7837,-455 435.7837,-461 435.7837,-461 435.7837,-501 435.7837,-501 435.7837,-507 429.7837,-513 423.7837,-513\"/>\n",
       "<text text-anchor=\"start\" x=\"347.2725\" y=\"-497.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"334.0483\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"323.9243\" y=\"-469.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [50, 0, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"331.3276\" y=\"-455.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M427.0343,-555.7677C419.4491,-544.6817 411.1823,-532.5994 403.5494,-521.4436\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"406.3933,-519.4019 397.8578,-513.1252 400.6161,-523.3547 406.3933,-519.4019\"/>\n",
       "<text text-anchor=\"middle\" x=\"393.2506\" y=\"-533.497\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"transparent\" stroke=\"#000000\" d=\"M599.3958,-520C599.3958,-520 466.3591,-520 466.3591,-520 460.3591,-520 454.3591,-514 454.3591,-508 454.3591,-508 454.3591,-454 454.3591,-454 454.3591,-448 460.3591,-442 466.3591,-442 466.3591,-442 599.3958,-442 599.3958,-442 605.3958,-442 611.3958,-448 611.3958,-454 611.3958,-454 611.3958,-508 611.3958,-508 611.3958,-514 605.3958,-520 599.3958,-520\"/>\n",
       "<text text-anchor=\"start\" x=\"462.1187\" y=\"-504.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal width (cm) ≤ 1.75</text>\n",
       "<text text-anchor=\"start\" x=\"504.2725\" y=\"-490.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"487.1553\" y=\"-476.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"477.0313\" y=\"-462.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 50, 50]</text>\n",
       "<text text-anchor=\"start\" x=\"479\" y=\"-448.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M481.0648,-555.7677C487.0853,-547.0798 493.5298,-537.7801 499.7571,-528.794\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"502.8209,-530.5176 505.64,-520.3046 497.0673,-526.5304 502.8209,-530.5176\"/>\n",
       "<text text-anchor=\"middle\" x=\"510.1009\" y=\"-540.7034\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.898039\" stroke=\"#000000\" d=\"M489.3569,-406C489.3569,-406 350.3979,-406 350.3979,-406 344.3979,-406 338.3979,-400 338.3979,-394 338.3979,-394 338.3979,-340 338.3979,-340 338.3979,-334 344.3979,-328 350.3979,-328 350.3979,-328 489.3569,-328 489.3569,-328 495.3569,-328 501.3569,-334 501.3569,-340 501.3569,-340 501.3569,-394 501.3569,-394 501.3569,-400 495.3569,-406 489.3569,-406\"/>\n",
       "<text text-anchor=\"start\" x=\"346.3877\" y=\"-390.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) ≤ 4.95</text>\n",
       "<text text-anchor=\"start\" x=\"383.4863\" y=\"-376.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.168</text>\n",
       "<text text-anchor=\"start\" x=\"378.0483\" y=\"-362.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 54</text>\n",
       "<text text-anchor=\"start\" x=\"367.9243\" y=\"-348.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 49, 5]</text>\n",
       "<text text-anchor=\"start\" x=\"366\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M493.9893,-441.7677C484.9337,-432.632 475.2076,-422.8198 465.8773,-413.407\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"468.363,-410.9429 458.8373,-406.3046 463.3914,-415.8708 468.363,-410.9429\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.976471\" stroke=\"#000000\" d=\"M715.3569,-406C715.3569,-406 576.3979,-406 576.3979,-406 570.3979,-406 564.3979,-400 564.3979,-394 564.3979,-394 564.3979,-340 564.3979,-340 564.3979,-334 570.3979,-328 576.3979,-328 576.3979,-328 715.3569,-328 715.3569,-328 721.3569,-328 727.3569,-334 727.3569,-340 727.3569,-340 727.3569,-394 727.3569,-394 727.3569,-400 721.3569,-406 715.3569,-406\"/>\n",
       "<text text-anchor=\"start\" x=\"572.3877\" y=\"-390.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) ≤ 4.85</text>\n",
       "<text text-anchor=\"start\" x=\"605.5933\" y=\"-376.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0425</text>\n",
       "<text text-anchor=\"start\" x=\"604.0483\" y=\"-362.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 46</text>\n",
       "<text text-anchor=\"start\" x=\"593.9243\" y=\"-348.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 45]</text>\n",
       "<text text-anchor=\"start\" x=\"596.2759\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>2&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M571.7656,-441.7677C580.8212,-432.632 590.5473,-422.8198 599.8775,-413.407\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"602.3635,-415.8708 606.9176,-406.3046 597.3919,-410.9429 602.3635,-415.8708\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.980392\" stroke=\"#000000\" d=\"M265.3958,-292C265.3958,-292 132.3591,-292 132.3591,-292 126.3591,-292 120.3591,-286 120.3591,-280 120.3591,-280 120.3591,-226 120.3591,-226 120.3591,-220 126.3591,-214 132.3591,-214 132.3591,-214 265.3958,-214 265.3958,-214 271.3958,-214 277.3958,-220 277.3958,-226 277.3958,-226 277.3958,-280 277.3958,-280 277.3958,-286 271.3958,-292 265.3958,-292\"/>\n",
       "<text text-anchor=\"start\" x=\"128.1187\" y=\"-276.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal width (cm) ≤ 1.65</text>\n",
       "<text text-anchor=\"start\" x=\"158.5933\" y=\"-262.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0408</text>\n",
       "<text text-anchor=\"start\" x=\"157.0483\" y=\"-248.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 48</text>\n",
       "<text text-anchor=\"start\" x=\"146.9243\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 47, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"145\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M344.131,-327.9272C324.6269,-317.8662 303.5145,-306.9757 283.5872,-296.6965\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"285.0168,-293.4957 274.5249,-292.0218 281.8077,-299.7168 285.0168,-293.4957\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.498039\" stroke=\"#000000\" d=\"M486.3958,-292C486.3958,-292 353.3591,-292 353.3591,-292 347.3591,-292 341.3591,-286 341.3591,-280 341.3591,-280 341.3591,-226 341.3591,-226 341.3591,-220 347.3591,-214 353.3591,-214 353.3591,-214 486.3958,-214 486.3958,-214 492.3958,-214 498.3958,-220 498.3958,-226 498.3958,-226 498.3958,-280 498.3958,-280 498.3958,-286 492.3958,-292 486.3958,-292\"/>\n",
       "<text text-anchor=\"start\" x=\"349.1187\" y=\"-276.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal width (cm) ≤ 1.55</text>\n",
       "<text text-anchor=\"start\" x=\"379.5933\" y=\"-262.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.4444</text>\n",
       "<text text-anchor=\"start\" x=\"381.9414\" y=\"-248.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"371.8174\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2, 4]</text>\n",
       "<text text-anchor=\"start\" x=\"370.2759\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M419.8774,-327.7677C419.8774,-319.6172 419.8774,-310.9283 419.8774,-302.4649\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"423.3775,-302.3046 419.8774,-292.3046 416.3775,-302.3047 423.3775,-302.3046\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#39e581\" stroke=\"#000000\" d=\"M111.6326,-171C111.6326,-171 12.1223,-171 12.1223,-171 6.1223,-171 .1223,-165 .1223,-159 .1223,-159 .1223,-119 .1223,-119 .1223,-113 6.1223,-107 12.1223,-107 12.1223,-107 111.6326,-107 111.6326,-107 117.6326,-107 123.6326,-113 123.6326,-119 123.6326,-119 123.6326,-159 123.6326,-159 123.6326,-165 117.6326,-171 111.6326,-171\"/>\n",
       "<text text-anchor=\"start\" x=\"33.2725\" y=\"-155.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"20.0483\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 47</text>\n",
       "<text text-anchor=\"start\" x=\"9.9243\" y=\"-127.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 47, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-113.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M151.7298,-213.7677C137.7541,-202.1383 122.4609,-189.4125 108.5175,-177.81\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"110.4096,-174.8312 100.484,-171.1252 105.9321,-180.2119 110.4096,-174.8312\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#8139e5\" stroke=\"#000000\" d=\"M244.5807,-171C244.5807,-171 153.1741,-171 153.1741,-171 147.1741,-171 141.1741,-165 141.1741,-159 141.1741,-159 141.1741,-119 141.1741,-119 141.1741,-113 147.1741,-107 153.1741,-107 153.1741,-107 244.5807,-107 244.5807,-107 250.5807,-107 256.5807,-113 256.5807,-119 256.5807,-119 256.5807,-159 256.5807,-159 256.5807,-165 250.5807,-171 244.5807,-171\"/>\n",
       "<text text-anchor=\"start\" x=\"170.2725\" y=\"-155.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"160.9414\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"150.8174\" y=\"-127.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 0, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"149.2759\" y=\"-113.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>4&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M198.8774,-213.7677C198.8774,-203.3338 198.8774,-192.0174 198.8774,-181.4215\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"202.3775,-181.1252 198.8774,-171.1252 195.3775,-181.1252 202.3775,-181.1252\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#8139e5\" stroke=\"#000000\" d=\"M377.5807,-171C377.5807,-171 286.1741,-171 286.1741,-171 280.1741,-171 274.1741,-165 274.1741,-159 274.1741,-159 274.1741,-119 274.1741,-119 274.1741,-113 280.1741,-107 286.1741,-107 286.1741,-107 377.5807,-107 377.5807,-107 383.5807,-107 389.5807,-113 389.5807,-119 389.5807,-119 389.5807,-159 389.5807,-159 389.5807,-165 383.5807,-171 377.5807,-171\"/>\n",
       "<text text-anchor=\"start\" x=\"303.2725\" y=\"-155.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"293.9414\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"283.8174\" y=\"-127.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 0, 3]</text>\n",
       "<text text-anchor=\"start\" x=\"282.2759\" y=\"-113.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M389.5928,-213.7677C380.9513,-202.573 371.5256,-190.3624 362.844,-179.1158\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"365.557,-176.9024 356.6758,-171.1252 360.0158,-181.1798 365.557,-176.9024\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.498039\" stroke=\"#000000\" d=\"M558.3569,-178C558.3569,-178 419.3979,-178 419.3979,-178 413.3979,-178 407.3979,-172 407.3979,-166 407.3979,-166 407.3979,-112 407.3979,-112 407.3979,-106 413.3979,-100 419.3979,-100 419.3979,-100 558.3569,-100 558.3569,-100 564.3569,-100 570.3569,-106 570.3569,-112 570.3569,-112 570.3569,-166 570.3569,-166 570.3569,-172 564.3569,-178 558.3569,-178\"/>\n",
       "<text text-anchor=\"start\" x=\"415.3877\" y=\"-162.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">petal length (cm) ≤ 5.45</text>\n",
       "<text text-anchor=\"start\" x=\"448.5933\" y=\"-148.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.4444</text>\n",
       "<text text-anchor=\"start\" x=\"450.9414\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"440.8174\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"435\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M443.6233,-213.7677C448.8275,-205.1694 454.3945,-195.9718 459.7812,-187.072\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"462.904,-188.672 465.0878,-178.3046 456.9155,-185.0473 462.904,-188.672\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<path fill=\"#39e581\" stroke=\"#000000\" d=\"M469.6326,-64C469.6326,-64 370.1223,-64 370.1223,-64 364.1223,-64 358.1223,-58 358.1223,-52 358.1223,-52 358.1223,-12 358.1223,-12 358.1223,-6 364.1223,0 370.1223,0 370.1223,0 469.6326,0 469.6326,0 475.6326,0 481.6326,-6 481.6326,-12 481.6326,-12 481.6326,-52 481.6326,-52 481.6326,-58 475.6326,-64 469.6326,-64\"/>\n",
       "<text text-anchor=\"start\" x=\"391.2725\" y=\"-48.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"381.9414\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"371.8174\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 2, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"366\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M463.5762,-99.7647C457.9223,-90.9971 451.9128,-81.678 446.2179,-72.8469\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"449.0138,-70.7242 440.6528,-64.2169 443.1309,-74.5178 449.0138,-70.7242\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<path fill=\"#8139e5\" stroke=\"#000000\" d=\"M602.5807,-64C602.5807,-64 511.1741,-64 511.1741,-64 505.1741,-64 499.1741,-58 499.1741,-52 499.1741,-52 499.1741,-12 499.1741,-12 499.1741,-6 505.1741,0 511.1741,0 511.1741,0 602.5807,0 602.5807,0 608.5807,0 614.5807,-6 614.5807,-12 614.5807,-12 614.5807,-52 614.5807,-52 614.5807,-58 608.5807,-64 602.5807,-64\"/>\n",
       "<text text-anchor=\"start\" x=\"528.2725\" y=\"-48.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"518.9414\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"508.8174\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 0, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"507.2759\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M513.812,-99.7647C519.384,-90.9971 525.3064,-81.678 530.9187,-72.8469\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"533.9934,-74.534 536.4032,-64.2169 528.0855,-70.7795 533.9934,-74.534\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.498039\" stroke=\"#000000\" d=\"M710.2193,-292C710.2193,-292 581.5356,-292 581.5356,-292 575.5356,-292 569.5356,-286 569.5356,-280 569.5356,-280 569.5356,-226 569.5356,-226 569.5356,-220 575.5356,-214 581.5356,-214 581.5356,-214 710.2193,-214 710.2193,-214 716.2193,-214 722.2193,-220 722.2193,-226 722.2193,-226 722.2193,-280 722.2193,-280 722.2193,-286 716.2193,-292 710.2193,-292\"/>\n",
       "<text text-anchor=\"start\" x=\"577.4565\" y=\"-276.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">sepal width (cm) ≤ 3.1</text>\n",
       "<text text-anchor=\"start\" x=\"605.5933\" y=\"-262.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.4444</text>\n",
       "<text text-anchor=\"start\" x=\"607.9414\" y=\"-248.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"597.8174\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"596.2759\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M645.8774,-327.7677C645.8774,-319.6172 645.8774,-310.9283 645.8774,-302.4649\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"649.3775,-302.3046 645.8774,-292.3046 642.3775,-302.3047 649.3775,-302.3046\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<path fill=\"#8139e5\" stroke=\"#000000\" d=\"M847.7837,-285C847.7837,-285 751.9712,-285 751.9712,-285 745.9712,-285 739.9712,-279 739.9712,-273 739.9712,-273 739.9712,-233 739.9712,-233 739.9712,-227 745.9712,-221 751.9712,-221 751.9712,-221 847.7837,-221 847.7837,-221 853.7837,-221 859.7837,-227 859.7837,-233 859.7837,-233 859.7837,-273 859.7837,-273 859.7837,-279 853.7837,-285 847.7837,-285\"/>\n",
       "<text text-anchor=\"start\" x=\"771.2725\" y=\"-269.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"758.0483\" y=\"-255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 43</text>\n",
       "<text text-anchor=\"start\" x=\"747.9243\" y=\"-241.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 0, 43]</text>\n",
       "<text text-anchor=\"start\" x=\"750.2759\" y=\"-227.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>12&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M698.8755,-327.7677C714.879,-315.9209 732.4195,-302.9364 748.3272,-291.1606\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"750.5252,-293.8881 756.4802,-285.1252 746.3604,-288.2619 750.5252,-293.8881\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<path fill=\"#8139e5\" stroke=\"#000000\" d=\"M691.5807,-171C691.5807,-171 600.1741,-171 600.1741,-171 594.1741,-171 588.1741,-165 588.1741,-159 588.1741,-159 588.1741,-119 588.1741,-119 588.1741,-113 594.1741,-107 600.1741,-107 600.1741,-107 691.5807,-107 691.5807,-107 697.5807,-107 703.5807,-113 703.5807,-119 703.5807,-119 703.5807,-159 703.5807,-159 703.5807,-165 697.5807,-171 691.5807,-171\"/>\n",
       "<text text-anchor=\"start\" x=\"617.2725\" y=\"-155.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"607.9414\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 2</text>\n",
       "<text text-anchor=\"start\" x=\"597.8174\" y=\"-127.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 0, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"596.2759\" y=\"-113.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M645.8774,-213.7677C645.8774,-203.3338 645.8774,-192.0174 645.8774,-181.4215\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"649.3775,-181.1252 645.8774,-171.1252 642.3775,-181.1252 649.3775,-181.1252\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<path fill=\"#39e581\" stroke=\"#000000\" d=\"M832.6326,-171C832.6326,-171 733.1223,-171 733.1223,-171 727.1223,-171 721.1223,-165 721.1223,-159 721.1223,-159 721.1223,-119 721.1223,-119 721.1223,-113 727.1223,-107 733.1223,-107 733.1223,-107 832.6326,-107 832.6326,-107 838.6326,-107 844.6326,-113 844.6326,-119 844.6326,-119 844.6326,-159 844.6326,-159 844.6326,-165 838.6326,-171 832.6326,-171\"/>\n",
       "<text text-anchor=\"start\" x=\"754.2725\" y=\"-155.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"744.9414\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">samples = 1</text>\n",
       "<text text-anchor=\"start\" x=\"734.8174\" y=\"-127.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">value = [0, 1, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"729\" y=\"-113.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\" fill=\"#000000\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>13&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M693.0251,-213.7677C707.0008,-202.1383 722.294,-189.4125 736.2374,-177.81\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"738.8227,-180.2119 744.2708,-171.1252 734.3453,-174.8312 738.8227,-180.2119\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x117064f90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample with iris Data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module sklearn.tree.tree in sklearn.tree:\n",
      "\n",
      "NAME\n",
      "    sklearn.tree.tree\n",
      "\n",
      "FILE\n",
      "    /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.py\n",
      "\n",
      "DESCRIPTION\n",
      "    This module gathers tree-based methods, including decision, regression and\n",
      "    randomized trees. Single and multi-output problems are both handled.\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.ClassifierMixin(__builtin__.object)\n",
      "        DecisionTreeClassifier(BaseDecisionTree, sklearn.base.ClassifierMixin)\n",
      "            ExtraTreeClassifier\n",
      "    sklearn.base.RegressorMixin(__builtin__.object)\n",
      "        DecisionTreeRegressor(BaseDecisionTree, sklearn.base.RegressorMixin)\n",
      "            ExtraTreeRegressor\n",
      "    BaseDecisionTree(abc.NewBase)\n",
      "        DecisionTreeClassifier(BaseDecisionTree, sklearn.base.ClassifierMixin)\n",
      "            ExtraTreeClassifier\n",
      "        DecisionTreeRegressor(BaseDecisionTree, sklearn.base.RegressorMixin)\n",
      "            ExtraTreeRegressor\n",
      "    \n",
      "    class DecisionTreeClassifier(BaseDecisionTree, sklearn.base.ClassifierMixin)\n",
      "     |  A decision tree classifier.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <tree>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : string, optional (default=\"gini\")\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |  \n",
      "     |  splitter : string, optional (default=\"best\")\n",
      "     |      The strategy used to choose the split at each node. Supported\n",
      "     |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "     |      the best random split.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=None)\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |          - If int, then consider `max_features` features at each split.\n",
      "     |          - If float, then `max_features` is a percentage and\n",
      "     |            `int(max_features * n_features)` features are considered at each\n",
      "     |            split.\n",
      "     |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |          - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_depth : int or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  class_weight : dict, list of dicts, \"balanced\" or None, optional (default=None)\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  min_impurity_split : float, optional (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  presort : bool, optional (default=False)\n",
      "     |      Whether to presort the data to speed up the finding of best splits in\n",
      "     |      fitting. For the default settings of a decision tree on large\n",
      "     |      datasets, setting this to true may slow down the training process.\n",
      "     |      When using either a smaller dataset or a restricted depth, this may\n",
      "     |      speed up the training.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : array of shape = [n_classes] or a list of such arrays\n",
      "     |      The classes labels (single output problem),\n",
      "     |      or a list of arrays of class labels (multi-output problem).\n",
      "     |  \n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances. The higher, the more important the\n",
      "     |      feature. The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance [4]_.\n",
      "     |  \n",
      "     |  max_features_ : int,\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (for single output problems),\n",
      "     |      or a list containing the number of classes for each\n",
      "     |      output (for multi-output problems).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  tree_ : Tree object\n",
      "     |      The underlying Tree object.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  DecisionTreeRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "     |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "     |  \n",
      "     |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "     |         Learning\", Springer, 2009.\n",
      "     |  \n",
      "     |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "     |         http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.model_selection import cross_val_score\n",
      "     |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      "     |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      "     |  >>> iris = load_iris()\n",
      "     |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      "     |  ...                             # doctest: +SKIP\n",
      "     |  ...\n",
      "     |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      "     |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DecisionTreeClassifier\n",
      "     |      BaseDecisionTree\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None, presort=False)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
      "     |      Build a decision tree classifier from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
      "     |          The indexes of the sorted training input samples. If many tree\n",
      "     |          are grown on the same dataset, this allows the ordering to be\n",
      "     |          cached between trees. If None, the data will be sorted here.\n",
      "     |          Don't use this parameter unless you know what to do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities of the input samples X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X, check_input=True)\n",
      "     |      Predict class probabilities of the input samples X.\n",
      "     |      \n",
      "     |      The predicted class probability is the fraction of samples of the same\n",
      "     |      class in a leaf.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Returns the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples,]\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class DecisionTreeRegressor(BaseDecisionTree, sklearn.base.RegressorMixin)\n",
      "     |  A decision tree regressor.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <tree>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  criterion : string, optional (default=\"mse\")\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"mse\" for the mean squared error, which is equal to variance\n",
      "     |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      "     |      absolute error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |  splitter : string, optional (default=\"best\")\n",
      "     |      The strategy used to choose the split at each node. Supported\n",
      "     |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      "     |      the best random split.\n",
      "     |  \n",
      "     |  max_features : int, float, string or None, optional (default=None)\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a percentage and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_depth : int or None, optional (default=None)\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int, float, optional (default=2)\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a percentage and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_samples_leaf : int, float, optional (default=1)\n",
      "     |      The minimum number of samples required to be at a leaf node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a percentage and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for percentages.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, optional (default=0.)\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int or None, optional (default=None)\n",
      "     |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, optional (default=None)\n",
      "     |      If int, random_state is the seed used by the random number generator;\n",
      "     |      If RandomState instance, random_state is the random number generator;\n",
      "     |      If None, the random number generator is the RandomState instance used\n",
      "     |      by `np.random`.\n",
      "     |  \n",
      "     |  min_impurity_split : float, optional (default=1e-7)\n",
      "     |      Threshold for early stopping in tree growth. If the impurity\n",
      "     |      of a node is below the threshold, the node is a leaf.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  presort : bool, optional (default=False)\n",
      "     |      Whether to presort the data to speed up the finding of best splits in\n",
      "     |      fitting. For the default settings of a decision tree on large\n",
      "     |      datasets, setting this to true may slow down the training process.\n",
      "     |      When using either a smaller dataset or a restricted depth, this may\n",
      "     |      speed up the training.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  feature_importances_ : array of shape = [n_features]\n",
      "     |      The feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the\n",
      "     |      (normalized) total reduction of the criterion brought\n",
      "     |      by that feature. It is also known as the Gini importance [4]_.\n",
      "     |  \n",
      "     |  max_features_ : int,\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  tree_ : Tree object\n",
      "     |      The underlying Tree object.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  DecisionTreeClassifier\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      "     |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      "     |  \n",
      "     |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      "     |         Learning\", Springer, 2009.\n",
      "     |  \n",
      "     |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      "     |         http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_boston\n",
      "     |  >>> from sklearn.model_selection import cross_val_score\n",
      "     |  >>> from sklearn.tree import DecisionTreeRegressor\n",
      "     |  >>> boston = load_boston()\n",
      "     |  >>> regressor = DecisionTreeRegressor(random_state=0)\n",
      "     |  >>> cross_val_score(regressor, boston.data, boston.target, cv=10)\n",
      "     |  ...                    # doctest: +SKIP\n",
      "     |  ...\n",
      "     |  array([ 0.61..., 0.57..., -0.34..., 0.41..., 0.75...,\n",
      "     |          0.07..., 0.29..., 0.33..., -1.42..., -1.77...])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DecisionTreeRegressor\n",
      "     |      BaseDecisionTree\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, presort=False)\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
      "     |      Build a decision tree regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (real numbers). Use ``dtype=np.float64`` and\n",
      "     |          ``order='C'`` for maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
      "     |          The indexes of the sorted training input samples. If many tree\n",
      "     |          are grown on the same dataset, this allows the ordering to be\n",
      "     |          cached between trees. If None, the data will be sorted here.\n",
      "     |          Don't use this parameter unless you know what to do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Returns the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples,]\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class ExtraTreeClassifier(DecisionTreeClassifier)\n",
      "     |  An extremely randomized tree classifier.\n",
      "     |  \n",
      "     |  Extra-trees differ from classic decision trees in the way they are built.\n",
      "     |  When looking for the best split to separate the samples of a node into two\n",
      "     |  groups, random splits are drawn for each of the `max_features` randomly\n",
      "     |  selected features and the best split among those is chosen. When\n",
      "     |  `max_features` is set 1, this amounts to building a totally random\n",
      "     |  decision tree.\n",
      "     |  \n",
      "     |  Warning: Extra-trees should only be used within ensemble methods.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <tree>`.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  ExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreeClassifier\n",
      "     |      DecisionTreeClassifier\n",
      "     |      BaseDecisionTree\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='gini', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', random_state=None, max_leaf_nodes=None, min_impurity_split=1e-07, class_weight=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DecisionTreeClassifier:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
      "     |      Build a decision tree classifier from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (class labels) as integers or strings.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. Splits are also\n",
      "     |          ignored if they would result in any single class carrying a\n",
      "     |          negative weight in either child node.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
      "     |          The indexes of the sorted training input samples. If many tree\n",
      "     |          are grown on the same dataset, this allows the ordering to be\n",
      "     |          cached between trees. If None, the data will be sorted here.\n",
      "     |          Don't use this parameter unless you know what to do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities of the input samples X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X, check_input=True)\n",
      "     |      Predict class probabilities of the input samples X.\n",
      "     |      \n",
      "     |      The predicted class probability is the fraction of samples of the same\n",
      "     |      class in a leaf.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape = [n_samples, n_classes], or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute `classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Returns the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples,]\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class ExtraTreeRegressor(DecisionTreeRegressor)\n",
      "     |  An extremely randomized tree regressor.\n",
      "     |  \n",
      "     |  Extra-trees differ from classic decision trees in the way they are built.\n",
      "     |  When looking for the best split to separate the samples of a node into two\n",
      "     |  groups, random splits are drawn for each of the `max_features` randomly\n",
      "     |  selected features and the best split among those is chosen. When\n",
      "     |  `max_features` is set 1, this amounts to building a totally random\n",
      "     |  decision tree.\n",
      "     |  \n",
      "     |  Warning: Extra-trees should only be used within ensemble methods.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <tree>`.\n",
      "     |  \n",
      "     |  See also\n",
      "     |  --------\n",
      "     |  ExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreeRegressor\n",
      "     |      DecisionTreeRegressor\n",
      "     |      BaseDecisionTree\n",
      "     |      abc.NewBase\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.feature_selection.from_model._LearntSelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      __builtin__.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, criterion='mse', splitter='random', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', random_state=None, min_impurity_split=1e-07, max_leaf_nodes=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset([])\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DecisionTreeRegressor:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)\n",
      "     |      Build a decision tree regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The training input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like, shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The target values (real numbers). Use ``dtype=np.float64`` and\n",
      "     |          ``order='C'`` for maximum efficiency.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples] or None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      X_idx_sorted : array-like, shape = [n_samples, n_features], optional\n",
      "     |          The indexes of the sorted training input samples. If many tree\n",
      "     |          are grown on the same dataset, this allows the ordering to be\n",
      "     |          cached between trees. If None, the data will be sorted here.\n",
      "     |          Don't use this parameter unless you know what to do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  apply(self, X, check_input=True)\n",
      "     |      Returns the index of the leaf that each sample is predicted as.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape = [n_samples,]\n",
      "     |          For each datapoint x in X, return the index of the leaf x\n",
      "     |          ends up in. Leaves are numbered within\n",
      "     |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      "     |          numbering.\n",
      "     |  \n",
      "     |  decision_path(self, X, check_input=True)\n",
      "     |      Return the decision path in the tree\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like or sparse matrix, shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse csr array, shape = [n_samples, n_nodes]\n",
      "     |          Return a node indicator matrix where non zero elements\n",
      "     |          indicates that the samples goes through the nodes.\n",
      "     |  \n",
      "     |  predict(self, X, check_input=True)\n",
      "     |      Predict class or regression value for X.\n",
      "     |      \n",
      "     |      For a classification model, the predicted class for each sample in X is\n",
      "     |      returned. For a regression model, the predicted value based on X is\n",
      "     |      returned.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      check_input : boolean, (default=True)\n",
      "     |          Allow to bypass several input checking.\n",
      "     |          Don't use this parameter unless you know what you do.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape = [n_samples] or [n_samples, n_outputs]\n",
      "     |          The predicted classes, or the predict values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseDecisionTree:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Return the feature importances.\n",
      "     |      \n",
      "     |      The importance of a feature is computed as the (normalized) total\n",
      "     |      reduction of the criterion brought by that feature.\n",
      "     |      It is also known as the Gini importance.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array, shape = [n_features]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : boolean, optional\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection.from_model._LearntSelectorMixin:\n",
      "     |  \n",
      "     |  transform(*args, **kwargs)\n",
      "     |      DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n",
      "     |      \n",
      "     |      Reduce X to its most important features.\n",
      "     |      \n",
      "     |              Uses ``coef_`` or ``feature_importances_`` to determine the most\n",
      "     |              important features.  For models with a ``coef_`` for each class, the\n",
      "     |              absolute sum over the classes is used.\n",
      "     |      \n",
      "     |              Parameters\n",
      "     |              ----------\n",
      "     |              X : array or scipy sparse matrix of shape [n_samples, n_features]\n",
      "     |                  The input samples.\n",
      "     |      \n",
      "     |              threshold : string, float or None, optional (default=None)\n",
      "     |                  The threshold value to use for feature selection. Features whose\n",
      "     |                  importance is greater or equal are kept while the others are\n",
      "     |                  discarded. If \"median\" (resp. \"mean\"), then the threshold value is\n",
      "     |                  the median (resp. the mean) of the feature importances. A scaling\n",
      "     |                  factor (e.g., \"1.25*mean\") may also be used. If None and if\n",
      "     |                  available, the object attribute ``threshold`` is used. Otherwise,\n",
      "     |                  \"mean\" is used by default.\n",
      "     |      \n",
      "     |              Returns\n",
      "     |              -------\n",
      "     |              X_r : array of shape [n_samples, n_selected_features]\n",
      "     |                  The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to X and y with optional parameters fit_params\n",
      "     |      and returns a transformed version of X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : numpy array of shape [n_samples, n_features]\n",
      "     |          Training set.\n",
      "     |      \n",
      "     |      y : numpy array of shape [n_samples]\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : numpy array of shape [n_samples, n_features_new]\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the regression\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      Best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DecisionTreeClassifier', 'DecisionTreeRegressor', 'ExtraTr...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tree.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Iris Plants Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML iris datasets.\\nhttp://archive.ics.uci.edu/ml/datasets/Iris\\n\\nThe famous Iris database, first used by Sir R.A Fisher\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\nReferences\\n----------\\n   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...\\n',\n",
       " 'data': array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "        [ 4.9,  3. ,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.3,  0.2],\n",
       "        [ 4.6,  3.1,  1.5,  0.2],\n",
       "        [ 5. ,  3.6,  1.4,  0.2],\n",
       "        [ 5.4,  3.9,  1.7,  0.4],\n",
       "        [ 4.6,  3.4,  1.4,  0.3],\n",
       "        [ 5. ,  3.4,  1.5,  0.2],\n",
       "        [ 4.4,  2.9,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5.4,  3.7,  1.5,  0.2],\n",
       "        [ 4.8,  3.4,  1.6,  0.2],\n",
       "        [ 4.8,  3. ,  1.4,  0.1],\n",
       "        [ 4.3,  3. ,  1.1,  0.1],\n",
       "        [ 5.8,  4. ,  1.2,  0.2],\n",
       "        [ 5.7,  4.4,  1.5,  0.4],\n",
       "        [ 5.4,  3.9,  1.3,  0.4],\n",
       "        [ 5.1,  3.5,  1.4,  0.3],\n",
       "        [ 5.7,  3.8,  1.7,  0.3],\n",
       "        [ 5.1,  3.8,  1.5,  0.3],\n",
       "        [ 5.4,  3.4,  1.7,  0.2],\n",
       "        [ 5.1,  3.7,  1.5,  0.4],\n",
       "        [ 4.6,  3.6,  1. ,  0.2],\n",
       "        [ 5.1,  3.3,  1.7,  0.5],\n",
       "        [ 4.8,  3.4,  1.9,  0.2],\n",
       "        [ 5. ,  3. ,  1.6,  0.2],\n",
       "        [ 5. ,  3.4,  1.6,  0.4],\n",
       "        [ 5.2,  3.5,  1.5,  0.2],\n",
       "        [ 5.2,  3.4,  1.4,  0.2],\n",
       "        [ 4.7,  3.2,  1.6,  0.2],\n",
       "        [ 4.8,  3.1,  1.6,  0.2],\n",
       "        [ 5.4,  3.4,  1.5,  0.4],\n",
       "        [ 5.2,  4.1,  1.5,  0.1],\n",
       "        [ 5.5,  4.2,  1.4,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 5. ,  3.2,  1.2,  0.2],\n",
       "        [ 5.5,  3.5,  1.3,  0.2],\n",
       "        [ 4.9,  3.1,  1.5,  0.1],\n",
       "        [ 4.4,  3. ,  1.3,  0.2],\n",
       "        [ 5.1,  3.4,  1.5,  0.2],\n",
       "        [ 5. ,  3.5,  1.3,  0.3],\n",
       "        [ 4.5,  2.3,  1.3,  0.3],\n",
       "        [ 4.4,  3.2,  1.3,  0.2],\n",
       "        [ 5. ,  3.5,  1.6,  0.6],\n",
       "        [ 5.1,  3.8,  1.9,  0.4],\n",
       "        [ 4.8,  3. ,  1.4,  0.3],\n",
       "        [ 5.1,  3.8,  1.6,  0.2],\n",
       "        [ 4.6,  3.2,  1.4,  0.2],\n",
       "        [ 5.3,  3.7,  1.5,  0.2],\n",
       "        [ 5. ,  3.3,  1.4,  0.2],\n",
       "        [ 7. ,  3.2,  4.7,  1.4],\n",
       "        [ 6.4,  3.2,  4.5,  1.5],\n",
       "        [ 6.9,  3.1,  4.9,  1.5],\n",
       "        [ 5.5,  2.3,  4. ,  1.3],\n",
       "        [ 6.5,  2.8,  4.6,  1.5],\n",
       "        [ 5.7,  2.8,  4.5,  1.3],\n",
       "        [ 6.3,  3.3,  4.7,  1.6],\n",
       "        [ 4.9,  2.4,  3.3,  1. ],\n",
       "        [ 6.6,  2.9,  4.6,  1.3],\n",
       "        [ 5.2,  2.7,  3.9,  1.4],\n",
       "        [ 5. ,  2. ,  3.5,  1. ],\n",
       "        [ 5.9,  3. ,  4.2,  1.5],\n",
       "        [ 6. ,  2.2,  4. ,  1. ],\n",
       "        [ 6.1,  2.9,  4.7,  1.4],\n",
       "        [ 5.6,  2.9,  3.6,  1.3],\n",
       "        [ 6.7,  3.1,  4.4,  1.4],\n",
       "        [ 5.6,  3. ,  4.5,  1.5],\n",
       "        [ 5.8,  2.7,  4.1,  1. ],\n",
       "        [ 6.2,  2.2,  4.5,  1.5],\n",
       "        [ 5.6,  2.5,  3.9,  1.1],\n",
       "        [ 5.9,  3.2,  4.8,  1.8],\n",
       "        [ 6.1,  2.8,  4. ,  1.3],\n",
       "        [ 6.3,  2.5,  4.9,  1.5],\n",
       "        [ 6.1,  2.8,  4.7,  1.2],\n",
       "        [ 6.4,  2.9,  4.3,  1.3],\n",
       "        [ 6.6,  3. ,  4.4,  1.4],\n",
       "        [ 6.8,  2.8,  4.8,  1.4],\n",
       "        [ 6.7,  3. ,  5. ,  1.7],\n",
       "        [ 6. ,  2.9,  4.5,  1.5],\n",
       "        [ 5.7,  2.6,  3.5,  1. ],\n",
       "        [ 5.5,  2.4,  3.8,  1.1],\n",
       "        [ 5.5,  2.4,  3.7,  1. ],\n",
       "        [ 5.8,  2.7,  3.9,  1.2],\n",
       "        [ 6. ,  2.7,  5.1,  1.6],\n",
       "        [ 5.4,  3. ,  4.5,  1.5],\n",
       "        [ 6. ,  3.4,  4.5,  1.6],\n",
       "        [ 6.7,  3.1,  4.7,  1.5],\n",
       "        [ 6.3,  2.3,  4.4,  1.3],\n",
       "        [ 5.6,  3. ,  4.1,  1.3],\n",
       "        [ 5.5,  2.5,  4. ,  1.3],\n",
       "        [ 5.5,  2.6,  4.4,  1.2],\n",
       "        [ 6.1,  3. ,  4.6,  1.4],\n",
       "        [ 5.8,  2.6,  4. ,  1.2],\n",
       "        [ 5. ,  2.3,  3.3,  1. ],\n",
       "        [ 5.6,  2.7,  4.2,  1.3],\n",
       "        [ 5.7,  3. ,  4.2,  1.2],\n",
       "        [ 5.7,  2.9,  4.2,  1.3],\n",
       "        [ 6.2,  2.9,  4.3,  1.3],\n",
       "        [ 5.1,  2.5,  3. ,  1.1],\n",
       "        [ 5.7,  2.8,  4.1,  1.3],\n",
       "        [ 6.3,  3.3,  6. ,  2.5],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 7.1,  3. ,  5.9,  2.1],\n",
       "        [ 6.3,  2.9,  5.6,  1.8],\n",
       "        [ 6.5,  3. ,  5.8,  2.2],\n",
       "        [ 7.6,  3. ,  6.6,  2.1],\n",
       "        [ 4.9,  2.5,  4.5,  1.7],\n",
       "        [ 7.3,  2.9,  6.3,  1.8],\n",
       "        [ 6.7,  2.5,  5.8,  1.8],\n",
       "        [ 7.2,  3.6,  6.1,  2.5],\n",
       "        [ 6.5,  3.2,  5.1,  2. ],\n",
       "        [ 6.4,  2.7,  5.3,  1.9],\n",
       "        [ 6.8,  3. ,  5.5,  2.1],\n",
       "        [ 5.7,  2.5,  5. ,  2. ],\n",
       "        [ 5.8,  2.8,  5.1,  2.4],\n",
       "        [ 6.4,  3.2,  5.3,  2.3],\n",
       "        [ 6.5,  3. ,  5.5,  1.8],\n",
       "        [ 7.7,  3.8,  6.7,  2.2],\n",
       "        [ 7.7,  2.6,  6.9,  2.3],\n",
       "        [ 6. ,  2.2,  5. ,  1.5],\n",
       "        [ 6.9,  3.2,  5.7,  2.3],\n",
       "        [ 5.6,  2.8,  4.9,  2. ],\n",
       "        [ 7.7,  2.8,  6.7,  2. ],\n",
       "        [ 6.3,  2.7,  4.9,  1.8],\n",
       "        [ 6.7,  3.3,  5.7,  2.1],\n",
       "        [ 7.2,  3.2,  6. ,  1.8],\n",
       "        [ 6.2,  2.8,  4.8,  1.8],\n",
       "        [ 6.1,  3. ,  4.9,  1.8],\n",
       "        [ 6.4,  2.8,  5.6,  2.1],\n",
       "        [ 7.2,  3. ,  5.8,  1.6],\n",
       "        [ 7.4,  2.8,  6.1,  1.9],\n",
       "        [ 7.9,  3.8,  6.4,  2. ],\n",
       "        [ 6.4,  2.8,  5.6,  2.2],\n",
       "        [ 6.3,  2.8,  5.1,  1.5],\n",
       "        [ 6.1,  2.6,  5.6,  1.4],\n",
       "        [ 7.7,  3. ,  6.1,  2.3],\n",
       "        [ 6.3,  3.4,  5.6,  2.4],\n",
       "        [ 6.4,  3.1,  5.5,  1.8],\n",
       "        [ 6. ,  3. ,  4.8,  1.8],\n",
       "        [ 6.9,  3.1,  5.4,  2.1],\n",
       "        [ 6.7,  3.1,  5.6,  2.4],\n",
       "        [ 6.9,  3.1,  5.1,  2.3],\n",
       "        [ 5.8,  2.7,  5.1,  1.9],\n",
       "        [ 6.8,  3.2,  5.9,  2.3],\n",
       "        [ 6.7,  3.3,  5.7,  2.5],\n",
       "        [ 6.7,  3. ,  5.2,  2.3],\n",
       "        [ 6.3,  2.5,  5. ,  1.9],\n",
       "        [ 6.5,  3. ,  5.2,  2. ],\n",
       "        [ 6.2,  3.4,  5.4,  2.3],\n",
       "        [ 5.9,  3. ,  5.1,  1.8]]),\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'],\n",
       "       dtype='|S10')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAID Tree ( Chi Square  Automatic Interaction Detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting CHAID\n",
      "  Downloading https://files.pythonhosted.org/packages/e7/2d/09be3a36c26efcdabcc7147cef3af50ccc59d4007b5ab283f176c0431ae5/CHAID-5.0.4.tar.gz\n",
      "Requirement already satisfied: cython in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from CHAID) (0.25.2)\n",
      "Requirement already satisfied: numpy in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from CHAID) (1.13.1)\n",
      "Requirement already satisfied: pandas in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from CHAID) (0.20.1)\n",
      "Collecting treelib (from CHAID)\n",
      "  Downloading https://files.pythonhosted.org/packages/f1/11/19621205d33cbd45a28740eee35754d0d28aba5d829ad20d1d1d2162bcb3/treelib-1.5.1.tar.gz\n",
      "Requirement already satisfied: pytest in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from CHAID) (3.0.7)\n",
      "Requirement already satisfied: scipy in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from CHAID) (0.19.0)\n",
      "Collecting savReaderWriter (from CHAID)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/2b/d7c2f50a5756cbf32f8a095c4950749753ad10f58a6301734225b0ac1ddc/savReaderWriter-3.4.2.tar.gz (50.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 50.9MB 329kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /Users/wenxuanzhang/.local/lib/python2.7/site-packages (from pandas->CHAID) (2.7.2)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/wenxuanzhang/.local/lib/python2.7/site-packages (from pandas->CHAID) (2018.3)\n",
      "Requirement already satisfied: py>=1.4.29 in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from pytest->CHAID) (1.4.33)\n",
      "Requirement already satisfied: setuptools in /Users/wenxuanzhang/.local/lib/python2.7/site-packages (from pytest->CHAID) (39.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wenxuanzhang/anaconda/lib/python2.7/site-packages (from python-dateutil->pandas->CHAID) (1.11.0)\n",
      "Building wheels for collected packages: CHAID, treelib, savReaderWriter\n",
      "  Running setup.py bdist_wheel for CHAID ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/wenxuanzhang/Library/Caches/pip/wheels/46/a2/5d/3d2656e54904d94d80f2d4c313e86cde8e2ed45e5b7fa4285f\n",
      "  Running setup.py bdist_wheel for treelib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/wenxuanzhang/Library/Caches/pip/wheels/db/ea/4e/d384c13ddd6c64110ca8ad4a3f7b05c4455d59678b0770fdcc\n",
      "  Running setup.py bdist_wheel for savReaderWriter ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/wenxuanzhang/Library/Caches/pip/wheels/cc/c5/c2/25171e344dff66668d68736623515b77ecda30dc399270c9ed\n",
      "Successfully built CHAID treelib savReaderWriter\n",
      "\u001b[31minstagramapi 1.0.2 has requirement requests==2.11.1, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mmoviepy 0.2.3.2 has requirement tqdm==4.11.2, but you'll have tqdm 4.19.8 which is incompatible.\u001b[0m\n",
      "\u001b[31minstagram-scraper 1.5.20 has requirement futures==2.2, but you'll have futures 3.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mgrpcio 1.9.1 has requirement protobuf>=3.5.0.post1, but you'll have protobuf 3.4.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: treelib, savReaderWriter, CHAID\n",
      "Successfully installed CHAID-5.0.4 savReaderWriter-3.4.2 treelib-1.5.1\n"
     ]
    }
   ],
   "source": [
    "# Implement the Tree Algorithm with Credit Default Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-7a12210815f3>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7a12210815f3>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    table(termCrosssell$housing)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Incorporate R code in markdonw](https://www.datacamp.com/community/blog/jupyter-notebook-r#gs.z0gxLNc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
